{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install --reinstall -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y libmagic-dev\n",
        "\n",
        "!pip install -Uq \"unstructured[all-docs]\" pillow lxml pillow\n",
        "!pip install -Uq chromadb tiktoken\n",
        "!pip install -Uq langchain langchain-community langchain-openai langchain-groq\n",
        "!pip install -Uq python_dotenv\n",
        "!pip install -Uq unstructured\n",
        "!pip install -Uq nltk\n",
        "!pip install -Uq langchain-groq\n",
        "!pip install -Uq langchain langchain-ollama\n",
        "!pip install -Uq langchain-huggingface\n",
        "!pip install -Uq gTTs\n",
        "!pip install -Uq gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEkm8-7z3m8J",
        "outputId": "9d8866aa-ad67-43e9-f782-dc42a9c0754d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 3s (93.7 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 1 reinstalled, 0 to remove and 54 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\n",
            "Fetched 186 kB in 1s (221 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 124652 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.6) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libmagic-dev is already the newest version (1:5.41-3ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.12.0 requires aiofiles<24.0,>=22.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unstructured-client 0.29.0 requires aiofiles>=24.1.0, but you have aiofiles 23.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "import nltk\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from base64 import b64decode\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "from langdetect import detect, DetectorFactory\n",
        "import uuid\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.schema.document import Document\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain_huggingface import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "rJ0G_0yqY8zW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-VRIRLvGVjm"
      },
      "source": [
        "# Multi-modal RAG with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rzLpOtmpHBpZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# keys for the services we will use\n",
        "\n",
        "os.environ[\"OLLAMA_API_KEY\"] = \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIItDAFT8xNkwkYNtNCL+58HRVaJs6d0NGd6a0dKP1BSt\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6_W6cAHiBbQ"
      },
      "source": [
        "## Extract the data\n",
        "\n",
        "Extract the elements of the PDF that we will be able to use in the retrieval process: text, images, tables, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B26qz5jiZBU"
      },
      "source": [
        "### Partition PDF tables, text, and images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuD0nt3IibiM",
        "outputId": "17174055-816c-46cb-c703-ef2825b9c61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "file_path = '/content/directions.pdf'\n",
        "\n",
        "chunks = partition_pdf(\n",
        "    filename=file_path,\n",
        "    infer_table_structure=True,            # extract tables\n",
        "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
        "    chunking_strategy=\"by_title\",          # or 'basic'\n",
        "    max_characters=10000,                  # defaults to 500\n",
        "    combine_text_under_n_chars=2000,       # defaults to 0\n",
        "    new_after_n_chars=6000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dzgSUFbJi3nO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e3dcfe-2101-4fcc-db67-0b21228a84a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"<class 'unstructured.documents.elements.CompositeElement'>\"}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# We get 2 types of elements from the partition_pdf function\n",
        "set([str(type(el)) for el in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xtqyBBGCp0Cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e17a43d-19b2-43ad-dec9-8541c2fb8d54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.NarrativeText at 0x793c01f80450>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c0bb0d650>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c04cc6bd0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbc550>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbf6d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbd750>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbf5d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbf4d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbef10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbf0d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbe9d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbe610>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbdd50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbd790>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dbc0d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d908d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d90a10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d90b10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d90c50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d90e90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d91290>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d91550>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d919d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d90090>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d91fd0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d92210>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d92690>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d928d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d92a50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d932d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d934d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d935d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d93910>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d93b50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01d93f50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd41d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd4650>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd4890>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd4d10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd4f50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd53d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd5550>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd5e90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd5f90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd6250>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd6450>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd68d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd6a90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd6f10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x793c01dd7150>]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Each CompositeElement containes a bunch of related elements.\n",
        "# This makes it easy to use these elements together in a RAG pipeline.\n",
        "\n",
        "chunks[0].metadata.orig_elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ee_xYLDqfzh"
      },
      "source": [
        "# Separate extracted elements into tables and text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "313HbwUtqd0x"
      },
      "outputs": [],
      "source": [
        "# separate tables from texts\n",
        "tables = []\n",
        "texts = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    if \"Table\" in str(type(chunk)):\n",
        "        tables.append(chunk)\n",
        "\n",
        "    if \"CompositeElement\" in str(type((chunk))):\n",
        "        texts.append(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQN_zabBq05G"
      },
      "source": [
        "## Summarize the data\n",
        "\n",
        "Create a summary of each element extracted from the PDF. This summary will be vectorized and used in the retrieval process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RA_88GCq7Qi"
      },
      "source": [
        "### Text and Table summaries\n",
        "\n",
        "We don't need a multimodal model to generate the summaries of the tables and the text. I will use open source models available on Groq."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGqNDTLTsF0U"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iPcJ7zRGrPi9"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"\n",
        "You are an assistant tasked with summarizing tables and text.\n",
        "Give a concise summary of the table or text.\n",
        "\n",
        "Respond only with the summary, no additionnal comment.\n",
        "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
        "Just give the summary as it is.\n",
        "\n",
        "Table or text chunk: {element}\n",
        "\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A6JXWSvQDLYd"
      },
      "outputs": [],
      "source": [
        "# Summarize text\n",
        "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 3})\n",
        "\n",
        "# Summarize tables\n",
        "tables_html = [table.metadata.text_as_html for table in tables]\n",
        "table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dlRVT53jIyc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5079e45c-7796-4a83-c023-b0fb8d88a2b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Directions around the campus are provided, including walking distances and directions to various locations such as buildings, stairs, elevators, and pedestrian crossings.',\n",
              " 'The text describes a series of directions to navigate between various buildings, rooms, and elevators within a campus. The directions involve walking, going up or down stairs, and using elevators to reach different locations. The text provides step-by-step instructions for each route, including distances and landmarks to help users navigate the campus.',\n",
              " 'The document describes walking directions and distances between various locations within a campus, including buildings, entrances, and landmarks such as fountains and a pedestrian cross.',\n",
              " 'The building layout is described with distances between various entrance/exit points, staircases, elevators, and rooms. Distances range from 2 to 150 meters.',\n",
              " 'The distances between various locations in the PRECIS building and its surroundings are as follows: \\n- 2 meters to PRECIS Stairs, \\n- 1 meter to PRECIS Stairs, \\n- 5 meters to Entrance/Exit, \\n- 100 meters to Campus Entrance - Iuliu Maniu (1), \\n- 240 meters to Campus Entrance - Iuliu Maniu (2), \\n- 3 meters to PR508.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "text_summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSvM9O13lKwN"
      },
      "source": [
        "## Load data and summaries to vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YGWGQQrlO8R"
      },
      "source": [
        "### Create the vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uBoykTba5LHY"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=embeddings)\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7XXoqX43tc_"
      },
      "source": [
        "### Load the summaries and link the to the original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MyB0TCoYGev0"
      },
      "outputs": [],
      "source": [
        "# Add texts\n",
        "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
        "summary_texts = [\n",
        "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_texts)\n",
        "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
        "\n",
        "# Add tables\n",
        "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
        "summary_tables = [\n",
        "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
        "]\n",
        "retriever.docstore.mset(list(zip(table_ids, tables)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC2KurqSKoAY"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wTERkQ9IKoiM"
      },
      "outputs": [],
      "source": [
        "def parse_docs(docs):\n",
        "    \"\"\"Process and separate text data\"\"\"\n",
        "    text = []\n",
        "    for doc in docs:\n",
        "        text.append(doc)\n",
        "    return {\"texts\": text}\n",
        "\n",
        "\n",
        "def build_prompt(kwargs):\n",
        "\n",
        "    docs_by_type = kwargs[\"context\"]\n",
        "    user_question = kwargs[\"question\"]\n",
        "\n",
        "    context_text = \"\"\n",
        "    if len(docs_by_type[\"texts\"]) > 0:\n",
        "        for text_element in docs_by_type[\"texts\"]:\n",
        "            context_text += text_element.text\n",
        "\n",
        "    # construct prompt with context (including images)\n",
        "    prompt_template = f\"\"\"\n",
        "    You are an AI assistant specialized in finding the best possible route between a destination and another.\n",
        "    Your task is to analyze a short a pdf that contains direction advices on how different locations connect to each other.\n",
        "\n",
        "    Specifically, provide the following details:\n",
        "      1. Offer the shortest path according to directions you have.\n",
        "      2. Use as few intermediate checkpoints as you can.\n",
        "      3. A classroom name is composed of two uppercased letters and three digits (example given: EC105).\n",
        "      4. If the user asks for a classroom you do not have information about, give an aproximate path. For example: the users asks for EC103, an aproximate path should be leading to EC105, because they are in the same building.\n",
        "      5. You will calculate the shortesc path in meters, not according to the number of intermediate locations.\n",
        "      6. If locatia A is connected to location B by a number of meters, then also location B is connected to location A by a number of meters.\n",
        "      7. When you give direction, you should specify when you turn left and right.\n",
        "      8. Make you do not repeat the yourself.\n",
        "      9. You will prompt only one possible route. The best one.\n",
        "      10. You do not need to specify the total distance.\n",
        "\n",
        "    Context: {context_text}\n",
        "    Question: {user_question}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
        "\n",
        "\n",
        "    return ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            HumanMessage(content=prompt_content),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": retriever | RunnableLambda(parse_docs),\n",
        "        \"question\": RunnablePassthrough(),\n",
        "    }\n",
        "    | RunnableLambda(build_prompt)\n",
        "    | ChatGroq(temperature=0.5, model=\"llama-3.2-11b-vision-preview\")\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain_with_sources = {\n",
        "    \"context\": retriever | RunnableLambda(parse_docs),\n",
        "    \"question\": RunnablePassthrough(),\n",
        "} | RunnablePassthrough().assign(\n",
        "    response=(\n",
        "        RunnableLambda(build_prompt)\n",
        "        | ChatGroq(temperature=0.5, model=\"llama-3.2-11b-vision-preview\")\n",
        "        | StrOutputParser()\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BCflSahvSNOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "407ab9ae-e36b-45b8-937a-6a42fc5c7e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To get from EC105 to EC Caffeteria, follow these directions:\n",
            "\n",
            "From EC105, go to the left to find the stairs (EC Stairs (2)). Go up the stairs to the first floor. Then, turn to the right twice. If you go forward, through the glass doors, you will find the cafeteria.\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke(\n",
        "    \"how do i get from EC105 to EC Caffeteria?\"\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a seed to ensure consistent language detection\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def chatbot(user_input):\n",
        "    response_text = chain.invoke(user_input)\n",
        "\n",
        "    # Detect the language of the response_text\n",
        "    detected_lang = detect(response_text)\n",
        "\n",
        "    # Create the audio file from the response\n",
        "    tts = gTTS(text=response_text, lang=detected_lang)\n",
        "    audio_file = \"response.mp3\"\n",
        "    tts.save(audio_file)\n",
        "\n",
        "    return response_text, audio_file\n",
        "\n",
        "# Define the Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"text\", \"audio\"],\n",
        "    title=\"Milly guides you\",\n",
        "    description=\"Type something and the chatbot will respond with text and audio!\",\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "B2zC1I4ySav0",
        "outputId": "cfc0d04a-0a87-4fc6-ad0d-3700ae34db4b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e7ed758da00f45c9a3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e7ed758da00f45c9a3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jct54JIUU3vC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}